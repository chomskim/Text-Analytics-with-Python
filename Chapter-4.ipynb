{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 4 Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_extractors import bow_extractor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print new_doc_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'and', u'beautiful', u'blue', u'cheese', u'is', u'love', u'sky', u'so', u'the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n",
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(features, feature_names)\n",
    "display_features(new_doc_features, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from feature_extractors import tfidf_transformer\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "    \n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "\n",
    "# show term frequencies\n",
    "display_features(tf, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # to smoothen idf later\n",
    "\n",
    "# show document frequencies\n",
    "display_features([df], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "# show inverse document frequencies\n",
    "display_features([np.round(idf, 2)], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.92  0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.51  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    1.92  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.22  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    1.92  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    1.22  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.92  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    1.51]]\n"
     ]
    }
   ],
   "source": [
    "# compute idf diagonal matrix  \n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "# print the idf diagonal matrix\n",
    "print np.round(idf, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "\n",
    "# show tfidf feature matrix\n",
    "display_features(np.round(tfidf, 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5   4.35  3.5   2.89]\n"
     ]
    }
   ],
   "source": [
    "# compute L2 norms \n",
    "norms = norm(tfidf, axis=1)\n",
    "\n",
    "# print norms for each document\n",
    "print np.round(norms, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "\n",
    "# show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute new doc term freqs from bow freqs\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "# compute tfidf using idf matrix from train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "\n",
    "# show new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import tfidf_extractor\n",
    "    \n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Word Vectorization Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in new_doc]                        \n",
    "\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, \n",
    "                               size=10,\n",
    "                               window=10,\n",
    "                               min_count=2,\n",
    "                               sample=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01608407 -0.04819566  0.04227461 -0.03011346  0.0254148   0.01728328\n",
      "  0.0155535   0.00774884 -0.02752112  0.01646519]\n"
     ]
    }
   ],
   "source": [
    " print model['sky']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0472235   0.01662185 -0.01221706 -0.04724348 -0.04384995  0.00193343\n",
      " -0.03163504 -0.03423524  0.02661656  0.03033725]\n"
     ]
    }
   ],
   "source": [
    "print model['blue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.006 -0.01   0.015 -0.014  0.004 -0.006 -0.024 -0.007 -0.001 -0.   ]\n",
      " [-0.008 -0.01   0.021 -0.019 -0.002 -0.002 -0.011  0.002  0.003 -0.001]\n",
      " [-0.003 -0.007  0.008 -0.02  -0.001 -0.004 -0.014 -0.015  0.002 -0.01 ]\n",
      " [-0.047  0.017 -0.012 -0.047 -0.044  0.002 -0.032 -0.034  0.027  0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import averaged_word_vectorizer\n",
    "\n",
    "\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                 model=model,\n",
    "                                                 num_features=10)\n",
    "print np.round(avg_word_vec_features, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.016 -0.016  0.015 -0.039 -0.009  0.01  -0.008 -0.013 -0.     0.023]]\n"
     ]
    }
   ],
   "source": [
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                    model=model,\n",
    "                                                    num_features=10)\n",
    "print np.round(nd_avg_word_vec_features, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.011 -0.011  0.014 -0.011  0.007 -0.007 -0.024 -0.008 -0.004 -0.004]\n",
      " [-0.    -0.014  0.028 -0.014  0.004 -0.003 -0.012  0.011 -0.001 -0.002]\n",
      " [-0.001 -0.008  0.007 -0.019  0.001 -0.004 -0.012 -0.018  0.001 -0.014]\n",
      " [-0.047  0.017 -0.012 -0.047 -0.044  0.002 -0.032 -0.034  0.027  0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print np.round(wt_tfidf_word_vec_features, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.012 -0.019  0.018 -0.038 -0.006  0.01  -0.006 -0.011 -0.003  0.023]]\n"
     ]
    }
   ],
   "source": [
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                     tfidf_vectors=nd_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print np.round(nd_wt_tfidf_word_vec_features, 3)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual counts: [('ham', 10), ('spam', 10)]\n",
      "Predicted counts: [('spam', 11), ('ham', 9)]\n"
     ]
    }
   ],
   "source": [
    "actual_labels = ['spam', 'ham', 'spam', 'spam', 'spam',\n",
    "               'ham', 'ham', 'spam', 'ham', 'spam',\n",
    "               'spam', 'ham', 'ham', 'ham', 'spam',\n",
    "               'ham', 'ham', 'spam', 'spam', 'ham']\n",
    "              \n",
    "predicted_labels = ['spam', 'spam', 'spam', 'ham', 'spam',\n",
    "                    'spam', 'ham', 'ham', 'spam', 'spam',\n",
    "                    'ham', 'ham', 'spam', 'ham', 'ham',\n",
    "                    'ham', 'spam', 'ham', 'spam', 'spam']\n",
    "                    \n",
    "ac = Counter(actual_labels)                     \n",
    "pc = Counter(predicted_labels)  \n",
    "\n",
    "print 'Actual counts:', ac.most_common()\n",
    "print 'Predicted counts:', pc.most_common()          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted:    \n",
      "                   spam ham\n",
      "Actual: spam          5   5\n",
      "        ham           6   4\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_true=actual_labels,\n",
    "                         y_pred=predicted_labels,\n",
    "                         labels=['spam','ham'])\n",
    "print pd.DataFrame(data=cm, \n",
    "                   columns=pd.MultiIndex(levels=[['Predicted:'],\n",
    "                                                 ['spam','ham']], \n",
    "                                         labels=[[0,0],[0,1]]), \n",
    "                   index=pd.MultiIndex(levels=[['Actual:'],\n",
    "                                               ['spam','ham']], \n",
    "                                       labels=[[0,0],[0,1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n",
      "Manually computed accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "positive_class = 'spam'\n",
    "\n",
    "true_positive = 5.\n",
    "false_positive = 6.\n",
    "false_negative = 5.\n",
    "true_negative = 4.\n",
    "\n",
    "accuracy = np.round(\n",
    "                metrics.accuracy_score(y_true=actual_labels,\n",
    "                                       y_pred=predicted_labels),2)\n",
    "accuracy_manual = np.round(\n",
    "                    (true_positive + true_negative) /\n",
    "                      (true_positive + true_negative +\n",
    "                       false_negative + false_positive),2)\n",
    "print 'Accuracy:', accuracy\n",
    "print 'Manually computed accuracy:', accuracy_manual                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.45\n",
      "Manually computed precision: 0.45\n"
     ]
    }
   ],
   "source": [
    "precision = np.round(\n",
    "                metrics.precision_score(y_true=actual_labels,\n",
    "                                        y_pred=predicted_labels,\n",
    "                                        pos_label=positive_class),2)\n",
    "precision_manual = np.round(\n",
    "                        (true_positive) /\n",
    "                        (true_positive + false_positive),2)\n",
    "print 'Precision:', precision\n",
    "print 'Manually computed precision:', precision_manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5\n",
      "Manually computed recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "recall = np.round(\n",
    "            metrics.recall_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2)\n",
    "recall_manual = np.round(\n",
    "                    (true_positive) /\n",
    "                    (true_positive + false_negative),2)\n",
    "print 'Recall:', recall\n",
    "print 'Manually computed recall:', recall_manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.48\n",
      "Manually computed F1 score: 0.47\n"
     ]
    }
   ],
   "source": [
    "f1_score = np.round(\n",
    "                metrics.f1_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2) \n",
    "f1_score_manual = np.round(\n",
    "                    (2 * precision * recall) /\n",
    "                    (precision + recall),2)\n",
    "print 'F1 score:', f1_score\n",
    "print 'Manually computed F1 score:', f1_score_manual                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Multi-Class Classification System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data\n",
    "    \n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, \n",
    "                                                        test_size=0.33, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "    \n",
    "    \n",
    "dataset = get_data()\n",
    "\n",
    "print dataset.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "Class label: 19\n",
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print 'Sample document:', corpus[10]\n",
    "print 'Class label:',labels[10]\n",
    "print 'Actual class label:', dataset.target_names[labels[10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_data_proportion=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from normalization import normalize_corpus\n",
    "\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)  \n",
    "\n",
    "''.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_extractors import bow_extractor, tfidf_extractor\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# bag of words features\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# tfidf features\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in norm_test_corpus]  \n",
    "# build word2vec model                   \n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)                                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print 'Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2)\n",
    "    print 'Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2)\n",
    "    print 'Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2)\n",
    "    print 'F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2)\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Precision: 0.72\n",
      "Recall: 0.67\n",
      "F1 Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Precision: 0.66\n",
      "Recall: 0.61\n",
      "F1 Score: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.78\n",
      "Recall: 0.72\n",
      "F1 Score: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with tfidf features                                           \n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Precision: 0.57\n",
      "Recall: 0.56\n",
      "F1 Score: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.56\n",
      "Recall: 0.53\n",
      "F1 Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism -> soc.religion.christian\n",
      "talk.politics.misc -> talk.politics.guns\n",
      "talk.religion.misc -> soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,20), columns=range(0,20))  \n",
    "\n",
    "class_names = dataset.target_names\n",
    "print class_names[0], '->', class_names[15]\n",
    "print class_names[18], '->', class_names[16]  \n",
    "print class_names[19], '->', class_names[15]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "I would like a list of Bible contadictions from those of you who dispite being free from Christianity are well versed in the Bible. \n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  They spent quite a bit of time on the wording of the Constitution.  They picked words whose meanings implied the intent.  We have already looked in the dictionary to define the word.  Isn't this sufficient?   But we were discussing it in relation to the death penalty.  And, the Constitution need not define each of the words within.  Anyone who doesn't know what cruel is can look in the dictionary (and we did).\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "Our Lord and Savior David Keresh has risen!   \tHe has been seen alive!   \tSpread the word!     --------------------------------------------------------------------------------\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  \"This is your god\" (from John Carpenter's \"They Live,\" natch)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 15:\n",
    "        print 'Actual Label:', class_names[label]\n",
    "        print 'Predicted Label:', class_names[predicted_label]\n",
    "        print 'Document:-'\n",
    "        print re.sub('\\n', ' ', document)\n",
    "        print\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "After the initial gun battle was over, they had 50 days to come out peacefully. They had their high priced lawyer, and judging by the posts here they had some public support. Can anyone come up with a rational explanation why the didn't come out (even after they negotiated coming out after the radio sermon) that doesn't include the Davidians wanting to commit suicide/murder/general mayhem?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Yesterday, the FBI was saying that at least three of the bodies had gunshot wounds, indicating that they were shot trying to escape the fire.  Today's paper quotes the medical examiner as saying that there is no evidence of gunshot wounds in any of the recovered bodies.  At the beginning of this siege, it was reported that while Koresh had a class III (machine gun) license, today's paper quotes the government as saying, no, they didn't have a license.  Today's paper reports that a number of the bodies were found with shoulder weapons next to them, as if they had been using them while dying -- which doesn't sound like the sort of action I would expect from a suicide.  Our government lies, as it tries to cover over its incompetence and negligence.  Why should I believe the FBI's claims about anything else, when we can see that they are LYING?  This system of government is beyond reform.\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  Well, for one thing most, if not all the Dividians (depending on whether they could show they acted in self-defense and there were no illegal weapons), could have gone on with their life as they were living it. No one was forcing them to give up their religion or even their legal weapons. The Dividians had survived a change in leadership before so even if Koresch himself would have been convicted and sent to jail, they still could have carried on.   I don't think the Dividians were insane, but I don't see a reason for mass suicide (if the fire was intentional set by some of the Dividians.) We also don't know that, if the fire was intentionally set from inside, was it a generally know plan or was this something only an inner circle knew about, or was it something two or three felt they had to do with or without Koresch's knowledge/blessing, etc.? I don't know much about Masada. Were some people throwing others over? Did mothers jump over with their babies in their arms?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "rja@mahogany126.cray.com (Russ Anderson) writes...     \tThe fact is that Koresh and his followers involved themselves \tin a gun battle to control the Mt Carmel complex. That is not \tin dispute. From what I remember of the trial, the authories \tcouldn't reasonably establish who fired first, the big reason \tbehind the aquittal.                    _____  _____                   \\\\\\\\\\\\/ ___/___________________   Mitchell S Todd  \\\\\\\\/ /                 _____/__________________________ ________________    \\\\/ / mst4298@zeus._____/.'.'.'.'.'.'.'.'.'.'.'.'_'_'_/ \\_____        \\__    / / tamu.edu  _____/.'.'.'.'.'.'.'.'.'.'.'.'.'_'_/     \\__________\\__  / /        _____/_'_'_'_'_'_'_'_'_'_'_'_'_'_'_/                 \\_ / /__________/                  \\/____/\\\\\\\\\\\\  \t\t\t \\\\\\\\\\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 18 and predicted_label == 16:\n",
    "        print 'Actual Label:', class_names[label]\n",
    "        print 'Predicted Label:', class_names[predicted_label]\n",
    "        print 'Document:-'\n",
    "        print re.sub('\\n', ' ', document)\n",
    "        print\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
