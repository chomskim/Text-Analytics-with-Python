{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 3 Processing and Understanding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n",
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SENTENCE TOKENIZATION\n",
    "\n",
    "# loading text corpora\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = 'We will discuss briefly about the basic syntax,\\\n",
    " structure and design philosophies. \\\n",
    " There is a defined hierarchical syntax for Python code which you should remember \\\n",
    " when writing code! Python is a really powerful programming language!'\n",
    "               \n",
    "# Total characters in Alice in Wonderland\n",
    "print len(alice)\n",
    "# First 100 characters in the corpus\n",
    "print alice[0:100]\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences :-\n",
      "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should remember  when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[u\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " u\"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\",\n",
      " u'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.',\n",
      " u\"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " u'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "## default sentence tokenizer\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print 'Total sentences in sample_text:', len(sample_sentences)\n",
    "print 'Sample text sentences :-'\n",
    "pprint(sample_sentences)\n",
    "print '\\nTotal sentences in alice:', len(alice_sentences)\n",
    "print 'First 5 sentences in alice:-'\n",
    "pprint(alice_sentences[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n",
      "\n",
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "## Other languages sentence tokenization\n",
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total characters in the corpus\n",
    "print len(german_text)\n",
    "# First 100 characters in the corpus\n",
    "print german_text[0:100]\n",
    "print\n",
    "\n",
    "# default sentence tokenizer \n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance  \n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "# verify the type of german_tokenizer\n",
    "# should be PunktSentenceTokenizer\n",
    "print type(german_tokenizer)\n",
    "\n",
    "# check if results of both tokenizers match\n",
    "# should be True\n",
    "print german_sentences_def == german_sentences\n",
    "# print first 5 sentences of the corpus\n",
    "for sent in german_sentences[0:5]:\n",
    "    print sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should remember  when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "## using PunktSentenceTokenizer for sentence tokenization\n",
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
      " ' There is a defined hierarchical syntax for Python code which you should remember  when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "## using RegexpTokenizer for sentence tokenization\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "            pattern=SENTENCE_TOKENS_PATTERN,\n",
    "            gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WORD TOKENIZATION\n",
    "\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# default word tokenizer\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print words       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# treebank word tokenizer\n",
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# regex word tokenizer\n",
    "TOKEN_PATTERN = r'\\w+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print word_indices\n",
    "print [sentence[start:end] for start, end in word_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# derived regex tokenizers\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print words\n",
    "\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.'],\n",
      "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
      "  ['@', '@']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
    "    return word_tokens\n",
    "    \n",
    "token_list = [tokenize_text(text)  for text in corpus]\n",
    "pprint(token_list)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You', 'll', 'learn', 'a', 'lot', 'in', 'the', 'book'], ['Python', 'is', 'an', 'amazing', 'language']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens\n",
    "    \n",
    "filtered_list_1 =  [filter(None,[remove_characters_after_tokenization(tokens) \n",
    "                                for tokens in sentence_tokens]) \n",
    "                    for sentence_tokens in token_list]\n",
    "print filtered_list_1\n",
    "print  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence\n",
    "    \n",
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) \n",
    "                    for sentence in corpus]    \n",
    "print filtered_list_2\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) \n",
    "                  for sentence in corpus]\n",
    "print cleaned_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence\n",
    "    \n",
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) \n",
    "                    for sentence in cleaned_corpus]    \n",
    "print expanded_corpus\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "# case conversion    \n",
    "print corpus[0].lower()\n",
    "print corpus[0].upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing', 'language', '!']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "    \n",
    "expanded_corpus_tokens = [tokenize_text(text)\n",
    "                          for text in expanded_corpus]    \n",
    "filtered_list_3 =  [[remove_stopwords(tokens) \n",
    "                        for tokens in sentence_tokens] \n",
    "                        for sentence_tokens in expanded_corpus_tokens]\n",
    "print filtered_list_3\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# removing repeated characters\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "print remove_repeated_characters(sample_sentence_tokens)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')\n",
    "\n",
    "print ps.stem('lying')\n",
    "\n",
    "print ps.stem('strange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# lancaster stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')\n",
    "\n",
    "print ls.stem('lying')\n",
    "\n",
    "print ls.stem('strange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "ly\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# regex stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "\n",
    "print rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')\n",
    "\n",
    "print rs.stem('lying')\n",
    "\n",
    "print rs.stem('strange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: (u'danish', u'dutch', u'english', u'finnish', u'french', u'german', u'hungarian', u'italian', u'norwegian', u'porter', u'portuguese', u'romanian', u'russian', u'spanish', u'swedish')\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print 'Supported Languages:', SnowballStemmer.languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'autobahn'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "ss.stem('autobahnen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'spring'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "ss.stem('springen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n",
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print wnl.lemmatize('cars', 'n')\n",
    "print wnl.lemmatize('men', 'n')\n",
    "\n",
    "# lemmatize verbs\n",
    "print wnl.lemmatize('running', 'v')\n",
    "print wnl.lemmatize('ate', 'v')\n",
    "\n",
    "# lemmatize adjectives\n",
    "print wnl.lemmatize('saddest', 'a')\n",
    "print wnl.lemmatize('fancier', 'a')\n",
    "\n",
    "# ineffective lemmatization\n",
    "print wnl.lemmatize('ate', 'n')\n",
    "print wnl.lemmatize('fancier', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def tokens(text): \n",
    "    \"\"\"\n",
    "    Get all words from the corpus\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "WORDS = tokens(file('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "# top 10 words in corpus\n",
    "print WORD_COUNTS.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually \n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}\n",
    "\n",
    "\n",
    "def edits0(word): \n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away \n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}\n",
    "\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs \n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) \n",
    "                for i in range(len(word)+1)]\n",
    "                \n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "    \n",
    "    \n",
    "def correct(word):\n",
    "    \"\"\"\n",
    "    Get the best correct spelling for the input word\n",
    "    \"\"\"\n",
    "    # Priority is for edit distance 0, then 1, then 2\n",
    "    # else defaults to the input word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "\n",
    "def correct_match(match):\n",
    "    \"\"\"\n",
    "    Spell-correct word in match, \n",
    "    and preserve proper upper/lower/title case.\n",
    "    \"\"\"\n",
    "    \n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        \"\"\"\n",
    "        Return the case-function appropriate \n",
    "        for text: upper, lower, title, or just str.:\n",
    "            \"\"\"\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "    \n",
    "def correct_text_generic(text):\n",
    "    \"\"\"\n",
    "    Correct all the words within a text, \n",
    "    returning the corrected text.\n",
    "    \"\"\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "print correct_text_generic('fianlly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('finally', 1.0)]\n",
      "[('flat', 0.85), ('float', 0.15)]\n"
     ]
    }
   ],
   "source": [
    "#pip install pattern\n",
    "\n",
    "from pattern.en import suggest\n",
    "\n",
    "print suggest('fianlly')\n",
    "print suggest('flaot')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Text Syntax and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pydot-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', u'DET'), ('brown', u'ADJ'), ('fox', u'NOUN'), ('is', u'VERB'), ('quick', u'ADJ'), ('and', u'CONJ'), ('he', u'PRON'), ('is', u'VERB'), ('jumping', u'VERB'), ('over', u'ADP'), ('the', u'DET'), ('lazy', u'ADJ'), ('dog', u'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "\n",
    "# recommended tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print tagged_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import tag\n",
    "tagged_sent = tag(sentence)\n",
    "print tagged_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "# building your own tagger\n",
    "\n",
    "# preparing the data\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145415819537\n"
     ]
    }
   ],
   "source": [
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "\n",
    "print dt.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print dt.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240391131765\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "print rt.evaluate(test_data)\n",
    "print rt.tag(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861361215994\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', None), ('dog', None)]\n",
      "0.134669377481\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n",
      "0.0806467228192\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print ut.evaluate(test_data)\n",
    "print ut.tag(tokens)\n",
    "\n",
    "print bt.evaluate(test_data)\n",
    "print bt.tag(tokens)\n",
    "\n",
    "print tt.evaluate(test_data)\n",
    "print tt.tag(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910155871817\n",
      "[('The', u'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', 'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "print ct.evaluate(test_data)        \n",
    "print ct.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930680607997\n",
      "[('The', u'DT'), ('brown', u'JJ'), ('fox', u'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'VBG')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "print nbt.evaluate(test_data)\n",
    "print nbt.tag(tokens)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pub/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/classify/maxent.py:1310: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/pub/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/classify/maxent.py:1312: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/pub/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/classify/maxent.py:1313: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n",
      "0.927001645851\n",
      "[('The', u'DT'), ('brown', u'NN'), ('fox', u'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'NN'), ('dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "# try this out for fun!\n",
    "met = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=MaxentClassifier.train)\n",
    "print met.evaluate(test_data)                           \n",
    "print met.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parsetree, Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence('The/DT/B-NP/O brown/JJ/I-NP/O fox/NN/I-NP/O is/VBZ/B-VP/O quick/JJ/B-ADJP/O and/CC/O/O he/PRP/B-NP/O is/VBZ/B-VP/O jumping/VBG/I-VP/O over/IN/B-PP/B-PNP the/DT/B-NP/I-PNP lazy/JJ/I-NP/I-PNP dog/NN/I-NP/I-PNP')]\n"
     ]
    }
   ],
   "source": [
    "tree = parsetree(sentence)\n",
    "print tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    print sentence_tree.chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP -> [(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN')]\n",
      "VP -> [(u'is', u'VBZ')]\n",
      "ADJP -> [(u'quick', u'JJ')]\n",
      "NP -> [(u'he', u'PRP')]\n",
      "VP -> [(u'is', u'VBZ'), (u'jumping', u'VBG')]\n",
      "PP -> [(u'over', u'IN')]\n",
      "NP -> [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print chunk.type, '->', [(word.string, word.type) \n",
    "                                 for word in chunk.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence, \n",
    "                              relations=True, \n",
    "                              lemmata=lemmatize)\n",
    "    return sentence_tree[0]\n",
    "\n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "    \n",
    "def process_sentence_tree(sentence_tree):\n",
    "    \n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        (item.type,\n",
    "                         [\n",
    "                             (w.string, w.type)\n",
    "                             for w in item.words\n",
    "                         ]\n",
    "                        )\n",
    "                        if type(item) == Chunk\n",
    "                        else ('-',\n",
    "                              [\n",
    "                                   (item.string, item.type)\n",
    "                              ]\n",
    "                             )\n",
    "                             for item in tree_constituents\n",
    "                    ]\n",
    "    \n",
    "    return processed_tree\n",
    "    \n",
    "def print_sentence_tree(sentence_tree):\n",
    "    \n",
    "\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "\n",
    "    tree = Tree('S', processed_tree )\n",
    "    print tree\n",
    "    \n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    \n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "    tree = Tree('S', processed_tree )\n",
    "    tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence('The/DT/B-NP/O/NP-SBJ-1 brown/JJ/I-NP/O/NP-SBJ-1 fox/NN/I-NP/O/NP-SBJ-1 is/VBZ/B-VP/O/VP-1 quick/JJ/B-ADJP/O/O and/CC/O/O/O he/PRP/B-NP/O/NP-SBJ-2 is/VBZ/B-VP/O/VP-2 jumping/VBG/I-VP/O/VP-2 over/IN/B-PP/B-PNP/O the/DT/B-NP/I-PNP/O lazy/JJ/I-NP/I-PNP/O dog/NN/I-NP/I-PNP/O')\n"
     ]
    }
   ],
   "source": [
    "t = create_sentence_tree(sentence)\n",
    "print t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'NP', [(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN')]),\n",
       " (u'VP', [(u'is', u'VBZ')]),\n",
       " (u'ADJP', [(u'quick', u'JJ')]),\n",
       " ('-', [(u'and', u'CC')]),\n",
       " (u'NP', [(u'he', u'PRP')]),\n",
       " (u'VP', [(u'is', u'VBZ'), (u'jumping', u'VBG')]),\n",
       " (u'PP', [(u'over', u'IN')]),\n",
       " (u'NP', [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = process_sentence_tree(t)\n",
    "pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "print_sentence_tree(t)\n",
    "visualize_sentence_tree(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "print train_data[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', u'DT'), (u'quick', u'JJ'), (u'fox', u'NN'), (u'jumped', u'VBD'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'the quick fox jumped over the lazy dog'\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from pattern.en import tag\n",
    "\n",
    "tagged_simple_sent = tag(simple_sentence)\n",
    "print tagged_simple_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "chink_grammar = \"\"\"\n",
    "NP: {<.*>+} # chunk everything as NP\n",
    "}<VBD|IN>+{\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = tag(sentence)\n",
    "print tagged_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  54.5%\n",
      "    Precision:     25.0%\n",
      "    Recall:        52.5%\n",
      "    F-Measure:     33.9%\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sentence)\n",
    "\n",
    "print c\n",
    "print rc.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print train_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', u'DT', u'B-NP'),\n",
       " (u'Lorillard', u'NNP', u'I-NP'),\n",
       " (u'spokewoman', u'NN', u'I-NP'),\n",
       " (u'said', u'VBD', u'O'),\n",
       " (u',', u',', u'O'),\n",
       " (u'``', u'``', u'O'),\n",
       " (u'This', u'DT', u'B-NP'),\n",
       " (u'is', u'VBZ', u'O'),\n",
       " (u'an', u'DT', u'B-NP'),\n",
       " (u'old', u'JJ', u'I-NP'),\n",
       " (u'story', u'NN', u'I-NP'),\n",
       " (u'.', u'.', u'O')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc = tree2conlltags(train_sent)\n",
    "wtc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(wtc)\n",
    "print tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.6%\n",
      "    Precision:     98.4%\n",
      "    Recall:       100.0%\n",
      "    F-Measure:     99.2%\n"
     ]
    }
   ],
   "source": [
    "ntc = NGramTagChunker(train_data)\n",
    "print ntc.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  is/VBZ\n",
      "  jumping/VBG\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "tree = ntc.parse(tagged_sentence)\n",
    "print tree\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print train_wsj_data[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%\n",
      "    Precision:     80.8%\n",
      "    Recall:        85.9%\n",
      "    F-Measure:     83.3%\n"
     ]
    }
   ],
   "source": [
    "tc = NGramTagChunker(train_wsj_data)\n",
    "print tc.evaluate(test_wsj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency-based Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install libgcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]<---The[]--->[]\n",
      "--------\n",
      "[]<---brown[]--->[]\n",
      "--------\n",
      "[]<---fox[]--->[]\n",
      "--------\n",
      "[]<---is[]--->[]\n",
      "--------\n",
      "[]<---quick[]--->[]\n",
      "--------\n",
      "[]<---and[]--->[]\n",
      "--------\n",
      "[]<---he[]--->[]\n",
      "--------\n",
      "[]<---is[]--->[]\n",
      "--------\n",
      "[]<---jumping[]--->[]\n",
      "--------\n",
      "[]<---over[]--->[]\n",
      "--------\n",
      "[]<---the[]--->[]\n",
      "--------\n",
      "[]<---lazy[]--->[]\n",
      "--------\n",
      "[]<---dog[]--->[]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "parsed_sent = parser(unicode(sentence))\n",
    "\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in parsed_sent:\n",
    "    print dependency_pattern.format(word=token.orth_, \n",
    "                                  w_type=token.dep_,\n",
    "                                  left=[t.orth_ \n",
    "                                            for t \n",
    "                                            in token.lefts],\n",
    "                                  right=[t.orth_ \n",
    "                                             for t \n",
    "                                             in token.rights])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like wrong result!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads stanford-parser-full-2016-10-31.zip from\n",
    "https://nlp.stanford.edu/software/lex-parser.shtml#Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set java path\n",
    "import os\n",
    "java_path = r'/usr/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(path_to_jar='/pub/nltk-parser/stanford-parser/stanford-parser.jar',\n",
    "                               path_to_models_jar='/pub/nltk-parser/stanford-parser/stanford-parser-3.7.0-models.jar')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"500pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 500.00 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 496,-388 496,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (quick)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.5,-347.799C178.5,-336.163 178.5,-320.548 178.5,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182,-307.175 178.5,-297.175 175,-307.175 182,-307.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"70.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.12,-260.902C141.668,-255.468 132.525,-249.241 124.5,-243 114.257,-235.034 103.642,-225.519 94.5625,-216.937\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.9768,-214.403 87.3388,-210.004 92.1294,-219.453 96.9768,-214.403\"/>\n",
       "<text text-anchor=\"middle\" x=\"139.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"142.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (is)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.215,-260.799C166.189,-248.932 159.411,-232.928 153.702,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.897,-218.019 149.774,-210.175 150.451,-220.749 156.897,-218.019\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (and)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.988,-260.799C191.153,-248.932 198.12,-232.928 203.987,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.242,-220.741 208.024,-210.175 200.823,-217.947 207.242,-220.741\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (jumping)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M203.593,-260.799C222.418,-247.895 248.378,-230.099 268.98,-215.978\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.175,-218.716 277.444,-210.175 267.217,-212.943 271.175,-218.716\"/>\n",
       "<text text-anchor=\"middle\" x=\"260.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"110.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (brown)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62.0007,-173.799C56.1371,-161.932 48.2291,-145.928 41.5688,-132.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.5545,-130.59 36.9867,-123.175 38.2788,-133.691 44.5545,-130.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"61\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.5946,-173.799C84.179,-161.932 91.7104,-145.928 98.0535,-132.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.326,-133.714 102.417,-123.175 94.9925,-130.733 101.326,-133.714\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"230.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (he)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M285.124,-173.944C279.656,-168.359 273.693,-162.037 268.5,-156 261.745,-148.148 254.771,-139.238 248.67,-131.136\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.447,-129.005 242.674,-123.065 245.828,-133.18 251.447,-129.005\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (is)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.5,-173.799C302.5,-162.163 302.5,-146.548 302.5,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"306,-133.175 302.5,-123.175 299,-133.175 306,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">aux</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node11\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (dog)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;13 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>9&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M318.082,-173.799C329.25,-161.471 344.462,-144.679 356.952,-130.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"359.821,-132.936 363.941,-123.175 354.633,-128.237 359.821,-132.936\"/>\n",
       "<text text-anchor=\"middle\" x=\"361.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"298.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (over)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>13&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.108,-86.799C351.251,-74.3561 335.059,-57.3644 321.851,-43.5044\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"324.3,-41.0003 314.867,-36.1754 319.232,-45.8294 324.3,-41.0003\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (the)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>13&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M379.5,-86.799C379.5,-75.1626 379.5,-59.5479 379.5,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383,-46.1754 379.5,-36.1754 376,-46.1755 383,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"388\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"459.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (lazy)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.689,-86.799C407.4,-74.3561 423.392,-57.3644 436.437,-43.5044\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"439.03,-45.8563 443.335,-36.1754 433.932,-41.0587 439.03,-45.8563\"/>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 14 nodes>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list(sdp.raw_parse(sentence))  \n",
    "\n",
    "result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'quick', u'JJ'), u'nsubj', (u'fox', u'NN')),\n",
       " ((u'fox', u'NN'), u'det', (u'The', u'DT')),\n",
       " ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')),\n",
       " ((u'quick', u'JJ'), u'cop', (u'is', u'VBZ')),\n",
       " ((u'quick', u'JJ'), u'cc', (u'and', u'CC')),\n",
       " ((u'quick', u'JJ'), u'conj', (u'jumping', u'VBG')),\n",
       " ((u'jumping', u'VBG'), u'nsubj', (u'he', u'PRP')),\n",
       " ((u'jumping', u'VBG'), u'aux', (u'is', u'VBZ')),\n",
       " ((u'jumping', u'VBG'), u'nmod', (u'dog', u'NN')),\n",
       " ((u'dog', u'NN'), u'case', (u'over', u'IN')),\n",
       " ((u'dog', u'NN'), u'det', (u'the', u'DT')),\n",
       " ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in result[0].triples()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "dep_tree = [parse.tree() for parse in result][0]\n",
    "print dep_tree\n",
    "dep_tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 12 productions\n",
      "  'fox' -> 'The'\n",
      "  'fox' -> 'brown'\n",
      "  'quick' -> 'fox'\n",
      "  'quick' -> 'is'\n",
      "  'quick' -> 'and'\n",
      "  'quick' -> 'jumping'\n",
      "  'jumping' -> 'he'\n",
      "  'jumping' -> 'is'\n",
      "  'jumping' -> 'dog'\n",
      "  'dog' -> 'over'\n",
      "  'dog' -> 'the'\n",
      "  'dog' -> 'lazy'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "dependency_rules = \"\"\"\n",
    "'fox' -> 'The' | 'brown'\n",
    "'quick' -> 'fox' | 'is' | 'and' | 'jumping'\n",
    "'jumping' -> 'he' | 'is' | 'dog'\n",
    "'dog' -> 'over' | 'the' | 'lazy'\n",
    "\"\"\"\n",
    "\n",
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print dependency_grammar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 12 productions\n",
      "  'fox' -> 'The'\n",
      "  'fox' -> 'brown'\n",
      "  'quick' -> 'fox'\n",
      "  'quick' -> 'is'\n",
      "  'quick' -> 'and'\n",
      "  'quick' -> 'jumping'\n",
      "  'jumping' -> 'he'\n",
      "  'jumping' -> 'is'\n",
      "  'jumping' -> 'dog'\n",
      "  'dog' -> 'over'\n",
      "  'dog' -> 'the'\n",
      "  'dog' -> 'lazy'\n",
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print dependency_grammar\n",
    "\n",
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "tree = res[0] \n",
    "print tree\n",
    "\n",
    "tree.draw()                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency-based Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "scp = StanfordParser(path_to_jar='/pub/nltk-parser/stanford-parser/stanford-parser.jar',\n",
    "                    path_to_models_jar='/pub/nltk-parser/stanford-parser/stanford-parser-3.7.0-models.jar')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (NP\n",
      "    (S\n",
      "      (S\n",
      "        (NP (DT The) (JJ brown) (NN fox))\n",
      "        (VP (VBZ is) (ADJP (JJ quick))))\n",
      "      (CC and)\n",
      "      (S\n",
      "        (NP (PRP he))\n",
      "        (VP\n",
      "          (VBZ is)\n",
      "          (VP\n",
      "            (VBG jumping)\n",
      "            (PP (IN over) (NP (DT the) (JJ lazy) (NN dog)))))))))\n"
     ]
    }
   ],
   "source": [
    "result = list(scp.raw_parse(sentence))\n",
    "print result[0]\n",
    "\n",
    "result[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print training_set[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VBZ -> 'cites',\n",
       " VBD -> 'spurned',\n",
       " PRN -> , ADVP-TMP ,,\n",
       " NNP -> 'ACCOUNT',\n",
       " JJ -> '36-day',\n",
       " NP-SBJ-2 -> NN,\n",
       " JJ -> 'unpublished',\n",
       " NP-SBJ-1 -> NNP,\n",
       " JJ -> 'elusive',\n",
       " NNS -> 'Lids']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-fdd2cfe07e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# get parse tree for sample sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviterbi_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pub/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/parse/viterbi.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The most likely constituent table.  This table specifies the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pub/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 631\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\"."
     ]
    }
   ],
   "source": [
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n",
    "\n",
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "print result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "# get tokens and their POS tags\n",
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n",
    "\n",
    "print tagged_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ-163 (DT The) (JJ brown) (NN fox))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (PRT (JJ quick))\n",
      "    (S\n",
      "      (CC and)\n",
      "      (NP-SBJ (PRP he))\n",
      "      (VP\n",
      "        (VBZ is)\n",
      "        (PP-1\n",
      "          (VBG jumping)\n",
      "          (NP (IN over) (DT the) (JJ lazy) (NN dog))))))) (p=2.02604e-48)\n"
     ]
    }
   ],
   "source": [
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print result[0]\n",
    "result[0].draw()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
